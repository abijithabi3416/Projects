{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4681053e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abiji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abiji\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data...\n",
      "Fetching data...\n",
      "Fetching data...\n",
      "Fetching data...\n",
      "Fetching data...\n",
      "Fetching data...\n",
      "Fetching data...\n",
      "Search For Publications or Authors: Climate\n",
      "Found 1 Matching Publications ' Climate ':\n",
      "\n",
      "Fetched Data:\n",
      "Publication Title: Nurturing a Climate of Innovation in a Didactic Educational System: A Case Study Exploring Leadership in Private Schools in Turkey\n",
      "Publication Date: 6 Oct 2022\n",
      "Publication Link: https://pureportal.coventry.ac.uk/en/publications/nurturing-a-climate-of-innovation-in-a-didactic-educational-syste\n",
      "Authors: Karakus, M., Clouder, D. L.\n",
      "Author Links: https://pureportal.coventry.ac.uk/en/persons/mehmet-karakus\n",
      "Author Links: https://pureportal.coventry.ac.uk/en/persons/deanne-clouder\n",
      "Filtered Text: nurtur climat innov didact educ system : case studi explor leadership privat school turkey karaku , m. , clouder , d. l. 6 oct 2022\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import tkinter as tk\n",
    "from collections import defaultdict\n",
    "from tkinter import Scrollbar, Listbox, END, RIGHT, Y\n",
    "\n",
    "def create_index():\n",
    "    return {}\n",
    "\n",
    "def extract_publications_data(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    all_publications = soup.find_all(\"div\", class_=\"rendering_researchoutput_portal-short\")\n",
    "\n",
    "    publications_data = []\n",
    "\n",
    "    for publication in all_publications:\n",
    "        idx = publication.index\n",
    "        title_element = publication.find(\"h3\", class_=\"title\")\n",
    "        title_text = title_element.a.text.strip() if title_element and     title_element.a else \"Title not available\"\n",
    "        publication_link = title_element.a[\"href\"] if title_element and title_element.a else \"Link not available\"\n",
    "\n",
    "        authors = publication.find_all(\"a\", class_=\"link person\")\n",
    "        author_text = \", \".join(author.text.strip() for author in authors) if authors else \"Author not available\"\n",
    "        \n",
    "        author_links = [author[\"href\"] for author in publication.find_all(\"a\", class_=\"link person\")]\n",
    "        author_links_text = \", \".join(author_links) if author_links else \"Author links not available\"\n",
    "\n",
    "        publication_date_element = publication.find(\"span\", class_=\"date\")\n",
    "        if publication_date_element:\n",
    "            publication_date_text = publication_date_element.text.strip()\n",
    "        else:\n",
    "            publication_date_text = \"Publication Date not available\"\n",
    "\n",
    "        publications_data.append({\n",
    "            \"Publication Title\": title_text,\n",
    "            \"Publication Date\": publication_date_text,\n",
    "            \"Publication Link\": publication_link,\n",
    "            \"Authors\": author_text,\n",
    "            \"Author Links\": author_links_text,\n",
    "        })\n",
    "\n",
    "    return publications_data\n",
    "\n",
    "def crawl_and_extract(base_url, num_pages):\n",
    "    publications_data = []\n",
    "    page_num = 0\n",
    "    \n",
    "    while page_num <= num_pages:\n",
    "        url = f\"{base_url}?page={page_num}\"\n",
    "        print(\"Fetching data...\")\n",
    "        page_data = extract_publications_data(url)\n",
    "        publications_data.extend(page_data)\n",
    "        page_num += 1\n",
    "    \n",
    "    return publications_data\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    terms = word_tokenize(text.lower())\n",
    "    filtered_text = [stemmer.stem(term) for term in terms if term not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "def build_inverted_index(publications_data):\n",
    "    term_to_documents = defaultdict(list)\n",
    "\n",
    "    n = 1\n",
    "    while n <= len(publications_data):\n",
    "        publication = publications_data[n - 1]\n",
    "        text_content = f\"{publication['Publication Title']} {publication['Authors']} {publication['Publication Date']}\"\n",
    "        filtered_text = preprocess_text(text_content)\n",
    "\n",
    "        for term in filtered_text.split():\n",
    "            term_to_documents[term].append(n)\n",
    "\n",
    "        publication[\"Filtered Text\"] = filtered_text\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    return term_to_documents\n",
    "\n",
    "\n",
    "def rank_matched_documents(query_terms, term_to_documents):\n",
    "    matched_docs = {}\n",
    "    term_index = 0\n",
    "\n",
    "    while term_index < len(query_terms):\n",
    "        term = query_terms[term_index]\n",
    "        if term in term_to_documents:\n",
    "            docs = term_to_documents[term]\n",
    "            doc_index = 0\n",
    "\n",
    "            while doc_index < len(docs):\n",
    "                doc = docs[doc_index]\n",
    "                matched_docs[doc] = matched_docs.get(doc, 0) + 1\n",
    "                doc_index += 1\n",
    "        \n",
    "        term_index += 1\n",
    "\n",
    "    ranked_docs = sorted(matched_docs.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [doc_id for doc_id, _ in ranked_docs]\n",
    "\n",
    "\n",
    "\n",
    "def search_publications(query, term_to_documents, publications_data):\n",
    "    query_terms = word_tokenize(query.lower())\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    query_terms = [stemmer.stem(term) for term in query_terms if term not in stop_words]\n",
    "    matched_docs = rank_matched_documents(query_terms, term_to_documents)\n",
    "    matched_publications = [publications_data[idx - 1] for idx in matched_docs]\n",
    "    if not matched_publications:\n",
    "        print(\"No publications found.\")\n",
    "        return\n",
    "    print(\"Found\", len(matched_publications), \"Matching Publications '\", query, \"':\\n\")\n",
    "    i = 0\n",
    "    while i < len(matched_publications):\n",
    "        publication = matched_publications[i]\n",
    "        print(\"Fetched Data:\")\n",
    "        for key, value in publication.items():\n",
    "            if key == \"Authors\":\n",
    "                print(key + \":\", value)\n",
    "            elif key == \"Author Links\":\n",
    "                links = value.split(\", \")\n",
    "                for link in links:\n",
    "                    print(key + \":\", link)\n",
    "            else:\n",
    "                print(key + \":\", value)\n",
    "        print(\"-\" * 40)\n",
    "        i += 1\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://pureportal.coventry.ac.uk/en/organisations/centre-global-learning/publications/\"\n",
    "    num_pages = 6  \n",
    "    publications_data = crawl_and_extract(base_url, num_pages)\n",
    "    term_to_documents = build_inverted_index(publications_data)\n",
    "    while True:\n",
    "        user_query = input(\"Search For Publications or Authors: \")\n",
    "        search_publications(user_query, term_to_documents, publications_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f95349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
